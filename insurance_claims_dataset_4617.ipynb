{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82dce34e"
      },
      "source": [
        "# Importing the required libraries"
      ],
      "id": "82dce34e"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9608467d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "9608467d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "500dbc7f"
      },
      "source": [
        "# Importing the dataset"
      ],
      "id": "500dbc7f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**These lines of code read in five CSV files into separate pandas dataframes.**\n",
        "\n",
        "\n",
        "*   traindf1 is read from the file 'Train_Claim.csv'\n",
        "\n",
        "*   traindf2 is read from the file 'Train_Demographics.csv'\n",
        "\n",
        "*  traindf3 is read from the file 'Train_Policy.csv'\n",
        "*   traindf4 is read from the file 'Train_Vehicle.csv'\n",
        "*   traindf5 is read from the file 'Traindata_with_Target.csv"
      ],
      "metadata": {
        "id": "13_d2Da-N0d0"
      },
      "id": "13_d2Da-N0d0"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4ead111e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "01b42d87-185c-49fa-9e8c-78321b6f07e4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b8775819c5fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#loading the datset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraindf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Train_Claim.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtraindf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Train_Demographics.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtraindf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Train_Policy.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtraindf4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Train_Vehicle.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m                 \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1254\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 19 fields in line 19353, saw 31\n"
          ]
        }
      ],
      "source": [
        "#loading the datset\n",
        "traindf1 = pd.read_csv('/content/Train_Claim.csv')\n",
        "traindf2 = pd.read_csv('/content/Train_Demographics.csv')\n",
        "traindf3 = pd.read_csv('/content/Train_Policy.csv')\n",
        "traindf4 = pd.read_csv('/content/Train_Vehicle.csv')\n",
        "traindf5 = pd.read_csv('/content/Traindata_with_Target.csv')"
      ],
      "id": "4ead111e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dea7e5a"
      },
      "outputs": [],
      "source": [
        "## To display the first few rows of the dataframe:\n",
        "traindf1"
      ],
      "id": "1dea7e5a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d11e67a4"
      },
      "outputs": [],
      "source": [
        "## To view the first 5 rows and shape \n",
        "traindf1.head()\n",
        "traindf1.shape"
      ],
      "id": "d11e67a4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32d572be"
      },
      "outputs": [],
      "source": [
        "## the number of missing values in each column of the traindf1 dataframe:\n",
        "traindf1.isnull().sum()"
      ],
      "id": "32d572be"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5c45829"
      },
      "outputs": [],
      "source": [
        "traindf2.head()\n",
        "traindf2.shape"
      ],
      "id": "a5c45829"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd31314c"
      },
      "outputs": [],
      "source": [
        "traindf2.isnull().sum()"
      ],
      "id": "fd31314c"
    },
    {
      "cell_type": "code",
      "source": [
        "display('traindf1', 'traindf2')"
      ],
      "metadata": {
        "id": "xGmQqEZa4KkB"
      },
      "id": "xGmQqEZa4KkB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a17e726"
      },
      "outputs": [],
      "source": [
        "traindf3.head()\n",
        "traindf3.shape"
      ],
      "id": "6a17e726"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbd4b493"
      },
      "outputs": [],
      "source": [
        "traindf3.isnull().sum()"
      ],
      "id": "bbd4b493"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7b921b4"
      },
      "outputs": [],
      "source": [
        "traindf4.head()\n",
        "traindf4.shape"
      ],
      "id": "c7b921b4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f882fbe1"
      },
      "outputs": [],
      "source": [
        "traindf5.isnull().sum()"
      ],
      "id": "f882fbe1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9bb20e6"
      },
      "outputs": [],
      "source": [
        "traindf5.head()\n",
        "traindf5.shape"
      ],
      "id": "c9bb20e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fd7566c"
      },
      "source": [
        "## merging the train data"
      ],
      "id": "2fd7566c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "merges five datasets (traindf1, traindf2, traindf3, traindf4, and traindf5) on a common column named \"CustomerID\" using the inner join."
      ],
      "metadata": {
        "id": "4zV4vA5TQNLS"
      },
      "id": "4zV4vA5TQNLS"
    },
    {
      "cell_type": "markdown",
      "source": [
        " The resulting merged_df dataframe contains only the rows where the CustomerID is present in all five datasets."
      ],
      "metadata": {
        "id": "R5OUXdJfQUB8"
      },
      "id": "R5OUXdJfQUB8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1686308"
      },
      "outputs": [],
      "source": [
        "# Merge the first two datasets on a common column\n",
        "merged_df = pd.merge(traindf1, traindf2, on='CustomerID',how = 'inner')\n",
        "\n",
        "# Merge the third dataset with the merged dataset on a common column\n",
        "merged_df = pd.merge(merged_df, traindf3, on='CustomerID' , how = 'inner')\n",
        "\n",
        "# Merge the fourth dataset with the merged dataset on a common column\n",
        "merged_df = pd.merge(merged_df, traindf4, on='CustomerID', how = 'inner')\n",
        "\n",
        "# Merge the fifth dataset with the merged dataset on a common column\n",
        "merged_df = pd.merge(merged_df, traindf5, on='CustomerID', how ='inner')\n"
      ],
      "id": "b1686308"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d028e722",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "merged_df"
      ],
      "id": "d028e722"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13bcb482"
      },
      "source": [
        "# here the target column is ReportedFraud"
      ],
      "id": "13bcb482"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b082a713"
      },
      "outputs": [],
      "source": [
        "merged_df.shape"
      ],
      "id": "b082a713"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c39d658a"
      },
      "outputs": [],
      "source": [
        "## to get the data types of each column in the DataFrame.\n",
        "merged_df.dtypes"
      ],
      "id": "c39d658a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f52f972"
      },
      "outputs": [],
      "source": [
        "## summary of the DataFrame, including the number of non-null values and data types for each column.\n",
        "merged_df.info()"
      ],
      "id": "8f52f972"
    },
    {
      "cell_type": "markdown",
      "source": [
        "provides descriptive statistics for the numerical columns in a DataFrame. It includes the count, mean, standard deviation, minimum value, 25th percentile, 50th percentile (median), 75th percentile, and maximum value."
      ],
      "metadata": {
        "id": "qjX9LesWRHYq"
      },
      "id": "qjX9LesWRHYq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80f81ad2"
      },
      "outputs": [],
      "source": [
        "##Look at the statistical data\n",
        "merged_df.describe()"
      ],
      "id": "80f81ad2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Data"
      ],
      "metadata": {
        "id": "y689d1bIElWG"
      },
      "id": "y689d1bIElWG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60ed0ec9"
      },
      "outputs": [],
      "source": [
        "merged_df.drop_duplicates(inplace=True)"
      ],
      "id": "60ed0ec9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "whether each row is a duplicate or not. "
      ],
      "metadata": {
        "id": "avhf9eiuR_bz"
      },
      "id": "avhf9eiuR_bz"
    },
    {
      "cell_type": "code",
      "source": [
        "## count the number of duplicates.\n",
        "print(merged_df.duplicated().sum())"
      ],
      "metadata": {
        "id": "pFsrQVwBRu5_"
      },
      "id": "pFsrQVwBRu5_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02dc1288"
      },
      "outputs": [],
      "source": [
        "## It looks like there are some missing values in the InsuredGender and Country columns.\n",
        "merged_df.isnull().sum()"
      ],
      "id": "02dc1288"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6b46902"
      },
      "outputs": [],
      "source": [
        "print(merged_df.InsuredGender.value_counts())"
      ],
      "id": "b6b46902"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0941e3bb"
      },
      "outputs": [],
      "source": [
        "merged_df['InsuredGender'].fillna(merged_df['InsuredGender'].mode()[0], inplace=True)\n"
      ],
      "id": "0941e3bb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7326ac71"
      },
      "outputs": [],
      "source": [
        "print(merged_df.Country.value_counts())"
      ],
      "id": "7326ac71"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99a13b23"
      },
      "outputs": [],
      "source": [
        "merged_df['Country'].fillna(merged_df['Country'].mode()[0], inplace=True)"
      ],
      "id": "99a13b23"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b68e7f93"
      },
      "outputs": [],
      "source": [
        "merged_df.isnull().sum()"
      ],
      "id": "b68e7f93"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fd4220a"
      },
      "outputs": [],
      "source": [
        "print(merged_df.ReportedFraud.value_counts())"
      ],
      "id": "8fd4220a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "N419qfrA23-1"
      },
      "id": "N419qfrA23-1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "015ecf4c"
      },
      "outputs": [],
      "source": [
        "list_unique_columns=[] #list to store the unique columns\n",
        "for i in merged_df.columns: ## each column name in the DataFrame merged_df\n",
        "    if len(merged_df[i].value_counts())==1: ##checks if the number of unique values in the column i is equal to 1 \n",
        "        list_unique_columns.append(i)\n",
        "for i in list_unique_columns:\n",
        "    merged_df.drop([i],axis=1,inplace=True)"
      ],
      "id": "015ecf4c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ed2305b"
      },
      "outputs": [],
      "source": [
        "#replace ? with nan\n",
        "merged_df=merged_df.replace('?',np.nan)"
      ],
      "id": "6ed2305b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "661b98aa"
      },
      "outputs": [],
      "source": [
        "# Check for question marks in dataframe\n",
        "for column in merged_df.columns:\n",
        "    print(column, merged_df[column][merged_df[column] == '?'].count())"
      ],
      "id": "661b98aa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcb3a876"
      },
      "outputs": [],
      "source": [
        "#missing value function\n",
        "def missing_data(data):\n",
        "    total = data.isnull().sum()\n",
        "    percent = (data.isnull().sum()/data.isnull().count()*100) ##o calculate the percentage of missing values in the dataframe. The code first sums the null values in the dataframe using isnull().sum() and then divides it by the total number of values in the dataframe (not null values) by using isnull().count(\n",
        "    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) ## concatenation two dataframes  total and percent\n",
        "    types = []\n",
        "    for col in data.columns:\n",
        "        dtype = str(data[col].dtype)\n",
        "        types.append(dtype)\n",
        "    tt['Types'] = types\n",
        "    return tt    \n",
        "\n",
        "missing_data(merged_df)['Percent'].sort_values(ascending=False)"
      ],
      "id": "bcb3a876"
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['PropertyDamage'] = pd.to_numeric(merged_df['PropertyDamage'], errors='coerce')\n",
        "merged_df['PoliceReport'] = pd.to_numeric(merged_df['PoliceReport'], errors='coerce')"
      ],
      "metadata": {
        "id": "ac9nkY0kUpQN"
      },
      "id": "ac9nkY0kUpQN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fill missing values in 'PropertyDamage' and 'PoliceReport' columns with their mean values\n",
        "merged_df['PropertyDamage'].fillna(merged_df['PropertyDamage'].mean(), inplace=True)\n",
        "merged_df['PoliceReport'].fillna(merged_df['PoliceReport'].mean(), inplace=True)"
      ],
      "metadata": {
        "id": "wUGIxj3kzi36"
      },
      "id": "wUGIxj3kzi36",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.drop('TypeOfCollission', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "2ctG7UFPZ2SK"
      },
      "id": "2ctG7UFPZ2SK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Numerical Features"
      ],
      "metadata": {
        "id": "XJaY7vcw3S7a"
      },
      "id": "XJaY7vcw3S7a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71909b36"
      },
      "outputs": [],
      "source": [
        "# Get numerical and categorical column names\n",
        "numerical_cols = merged_df.select_dtypes(include='number').columns.tolist()\n",
        "print(\"Numerical columns:\", numerical_cols)\n"
      ],
      "id": "71909b36"
    },
    {
      "cell_type": "code",
      "source": [
        "# assuming 'df' is the pandas DataFrame containing the data\n",
        "numerical_cols = ['IncidentTime', 'NumberOfVehicles', 'BodilyInjuries', 'AmountOfInjuryClaim', 'AmountOfPropertyClaim', 'AmountOfVehicleDamage', 'InsuredAge', 'InsuredZipCode', 'CapitalGains', 'CapitalLoss', 'InsurancePolicyNumber', 'CustomerLoyaltyPeriod', 'Policy_Deductible', 'PolicyAnnualPremium', 'UmbrellaLimit']\n",
        "\n",
        "unique_counts = merged_df[numerical_cols].nunique()\n",
        "print(unique_counts)"
      ],
      "metadata": {
        "id": "e4kYmQmFt4oO"
      },
      "id": "e4kYmQmFt4oO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb317caa"
      },
      "outputs": [],
      "source": [
        "# Create box plots for each numerical column\n",
        "for col in numerical_cols:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.boxplot(merged_df[col])\n",
        "    plt.title(col)\n",
        "    plt.ylabel('Value')\n",
        "    plt.show()"
      ],
      "id": "cb317caa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c204cdb"
      },
      "outputs": [],
      "source": [
        "# Create a box plot for each numeric column\n",
        "merged_df.boxplot(column=list(merged_df.select_dtypes(include='number').columns))\n",
        "\n",
        "# Calculate z-scores for all numerical variables\n",
        "z_scores = merged_df.select_dtypes(include=['float64', 'int64']).apply(lambda x: (x - x.mean()) / x.std())\n",
        "\n",
        "# Identify outliers based on z-scores greater than 3 or less than -3\n",
        "outliers = (z_scores > 3) | (z_scores < -3)\n",
        "\n",
        "# Print the number of outliers for each variable\n",
        "print(outliers.sum())"
      ],
      "id": "2c204cdb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52aeb132"
      },
      "outputs": [],
      "source": [
        "# Define a list of columns to replace outliers\n",
        "cols_to_replace = ['AmountOfInjuryClaim', 'AmountOfPropertyClaim', 'InsuredAge',\n",
        "                   'CapitalLoss', 'PolicyAnnualPremium', 'UmbrellaLimit']\n",
        "\n",
        "# Define a function to replace outliers with the median value\n",
        "def replace_outliers(merged_df, col):\n",
        "    q1 = merged_df[col].quantile(0.25)\n",
        "    q3 = merged_df[col].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_tail = q1 - 1.5 * iqr\n",
        "    upper_tail = q3 + 1.5 * iqr\n",
        "    med = np.median(merged_df[col])\n",
        "    for i in merged_df[col]:\n",
        "        if i > upper_tail or i < lower_tail:\n",
        "            merged_df[col] = merged_df[col].replace(i, med)\n",
        "    return merged_df\n",
        "\n",
        "# Replace outliers for each column in the list\n",
        "for col in cols_to_replace:\n",
        "    merged_df = replace_outliers(merged_df, col)\n",
        "\n",
        "# Plot the boxplots for each column after outlier treatment\n",
        "fig, axs = plt.subplots(5, 2, figsize=(20, 30))\n",
        "axs = axs.ravel()\n",
        "for i, col in enumerate(cols_to_replace):\n",
        "    sns.boxplot(x=col, data=merged_df, palette=\"Reds_r\", ax=axs[i])\n",
        "    axs[i].set_title(\"Box Plot for \"+col+\" after median imputation\")\n",
        "plt.show()"
      ],
      "id": "52aeb132"
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by 'InsuredGender' and create a histogram of 'PolicyAnnualPremium'\n",
        "sns.histplot(data=merged_df, x='PolicyAnnualPremium', hue='InsuredGender', kde=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "brIB65nIlo45"
      },
      "id": "brIB65nIlo45",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78548427"
      },
      "outputs": [],
      "source": [
        "merged_df.drop('UmbrellaLimit', axis=1, inplace=True)\n"
      ],
      "id": "78548427"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a3010c7"
      },
      "outputs": [],
      "source": [
        "merged_df['NetCapitalGain'] = merged_df['CapitalGains'] - merged_df['CapitalLoss']\n"
      ],
      "id": "8a3010c7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1af308c4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(merged_df['NetCapitalGain'], bins=50)\n",
        "plt.xlabel('Net Capital Gain')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "id": "1af308c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07e81ae1"
      },
      "outputs": [],
      "source": [
        "plt.scatter(merged_df['PolicyAnnualPremium'], merged_df['NetCapitalGain'])\n",
        "plt.xlabel('Policy Annual Premium')\n",
        "plt.ylabel('Net Capital Gain')\n",
        "plt.show()\n"
      ],
      "id": "07e81ae1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a78e87bc"
      },
      "outputs": [],
      "source": [
        "corr_coef = merged_df['PolicyAnnualPremium'].corr(merged_df['NetCapitalGain'])\n",
        "print(\"Correlation coefficient between PolicyAnnualPremium and NetCapitalGain:\", corr_coef)\n"
      ],
      "id": "a78e87bc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a scatter plot with a regression line that shows the relationship between 'PolicyAnnualPremium' and 'NetCapitalGain', as well as the correlation coefficient between the two variables."
      ],
      "metadata": {
        "id": "3-FyENHvf9-s"
      },
      "id": "3-FyENHvf9-s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3ad747a"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "corr_coef = merged_df['PolicyAnnualPremium'].corr(merged_df['NetCapitalGain'])  ## calculates the correlation coefficient between 'PolicyAnnualPremium' and 'NetCapitalGain' using the corr() method\n",
        "\n",
        "sns.regplot(x='PolicyAnnualPremium', y='NetCapitalGain', data=merged_df, line_kws={'color': 'red'})## creates a scatter plot with a regression line using Seaborn's regplot() function. The x and y parameters specify the variables to be plotted, and the data parameter specifies the DataFrame to use for the plot. The line_kws parameter specifies the color of the regression line.\n",
        "plt.xlabel('Policy Annual Premium') ## sets the x-axis label of the plot.\n",
        "plt.ylabel('Net Capital Gain') ##sets the y-axis label of the plot.\n",
        "plt.title('Correlation between Policy Annual Premium and Net Capital Gain (corr = {:.2f})'.format(corr_coef)) ##sets the title of the plot and includes the correlation coefficient value using string formatting.\n",
        "plt.show()\n"
      ],
      "id": "f3ad747a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorical Features"
      ],
      "metadata": {
        "id": "YxZQ0AdR3edJ"
      },
      "id": "YxZQ0AdR3edJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f46e2d29"
      },
      "outputs": [],
      "source": [
        "categorical_cols = merged_df.select_dtypes(include=['object', 'category']).columns.tolist()  ##selects all columns with data types that are either object or category. These are the data types typically used to represent categorical data.\n",
        "print(\"Categorical columns:\", categorical_cols)  ##prints the list of categorical column names."
      ],
      "id": "f46e2d29"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the categorical columns\n",
        "categorical_cols = ['CustomerID', 'DateOfIncident', 'TypeOfIncident', \n",
        "                    'SeverityOfIncident', 'AuthoritiesContacted', 'IncidentState', 'IncidentCity', \n",
        "                    'IncidentAddress', 'Witnesses', \n",
        "                    'InsuredGender', 'InsuredEducationLevel', 'InsuredOccupation', \n",
        "                    'InsuredHobbies', 'DateOfPolicyCoverage', 'InsurancePolicyState', 'Policy_CombinedSingleLimit', \n",
        "                    'InsuredRelationship', 'VehicleAttribute', 'VehicleAttributeDetails', 'ReportedFraud']"
      ],
      "metadata": {
        "id": "Y8uMEjE5sGvc"
      },
      "id": "Y8uMEjE5sGvc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = ['CustomerID', 'DateOfIncident', 'IncidentAddress', 'Witnesses','InsuredHobbies','DateOfPolicyCoverage', 'Policy_CombinedSingleLimit']\n",
        "\n",
        "merged_df = merged_df.drop(columns=columns_to_drop)"
      ],
      "metadata": {
        "id": "T0Q_YpnPAQQ2"
      },
      "id": "T0Q_YpnPAQQ2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_df.ReportedFraud.value_counts())"
      ],
      "metadata": {
        "id": "kMhbwin7He3n"
      },
      "id": "kMhbwin7He3n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of values in the ReportedFraud column\n",
        "fraud_counts = merged_df['ReportedFraud'].value_counts()\n",
        "\n",
        "# Create a bar plot of the counts\n",
        "plt.bar(fraud_counts.index, fraud_counts.values)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Reported Fraud')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Number of Reported Fraud Cases')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xTjmHs9ZIbPX"
      },
      "id": "xTjmHs9ZIbPX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6jVzKAEHnfLt"
      },
      "id": "6jVzKAEHnfLt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(merged_df.InsuredGender, merged_df.ReportedFraud).plot(kind='bar')\n",
        "plt.title('Number of Reported Fraud Cases')"
      ],
      "metadata": {
        "id": "jvHmOpqYI5TS"
      },
      "id": "jvHmOpqYI5TS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3d5o6g_Lm0v"
      },
      "outputs": [],
      "source": [
        "# Select categorical columns to plot\n",
        "cat_cols = ['TypeOfIncident', 'SeverityOfIncident', 'AuthoritiesContacted', 'InsuredGender', 'InsuredEducationLevel']\n",
        "\n",
        "# Plot pie chart for each categorical column\n",
        "for col in cat_cols:\n",
        "    plt.figure()\n",
        "    merged_df[col].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
        "    plt.title(col)\n",
        "    plt.axis('equal')\n",
        "    plt.show()\n"
      ],
      "id": "c3d5o6g_Lm0v"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary to map category values to numerical labels\n",
        "label_map = {} ## We create an empty dictionary to store the names of the labels\n",
        "label_count = 0 ## the label count initialized to 0.\n",
        "\n",
        "\n",
        "for col in categorical_cols: ##for every column (col) in the categorical column (categorical_cols) list\n",
        "    \n",
        "    # If the column is not in the columns to drop list\n",
        "    if col not in columns_to_drop:\n",
        "        \n",
        "        # Get unique values in the column\n",
        "        unique_vals = merged_df[col].unique()\n",
        "        \n",
        "        # Loop through each unique value\n",
        "        for val in unique_vals:\n",
        "            \n",
        "            # If the value has not been seen before, assign a new label\n",
        "            if val not in label_map:\n",
        "                label_map[val] = label_count\n",
        "                label_count += 1\n",
        "                \n",
        "        # Replace categorical values in the column with their corresponding numerical labels\n",
        "        merged_df[col] = merged_df[col].apply(lambda x: label_map[x]) "
      ],
      "metadata": {
        "id": "2FCjcNYEwEjB"
      },
      "id": "2FCjcNYEwEjB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3KB9eHI40lEP"
      },
      "id": "3KB9eHI40lEP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d5750a7"
      },
      "source": [
        "## loading the test data"
      ],
      "id": "1d5750a7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec311eb6"
      },
      "outputs": [],
      "source": [
        "testdf1 = pd.read_csv('/content/Test.csv')\n",
        "testdf2 = pd.read_csv('/content/Test_Claim.csv')\n",
        "testdf3 = pd.read_csv('/content/Test_Demographics.csv')\n",
        "testdf4 = pd.read_csv('/content/Test_Policy.csv')\n",
        "testdf5 = pd.read_csv('/content/Test_Vehicle.csv')"
      ],
      "id": "ec311eb6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5ef45c8"
      },
      "outputs": [],
      "source": [
        "testdf1.head()"
      ],
      "id": "f5ef45c8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "088c38ec"
      },
      "outputs": [],
      "source": [
        "testdf1.shape"
      ],
      "id": "088c38ec"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c16e51a4"
      },
      "outputs": [],
      "source": [
        "testdf2.shape"
      ],
      "id": "c16e51a4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5941ab2e"
      },
      "outputs": [],
      "source": [
        "testdf3.shape"
      ],
      "id": "5941ab2e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9f1137f"
      },
      "outputs": [],
      "source": [
        "testdf4.shape"
      ],
      "id": "e9f1137f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a8abaa6"
      },
      "outputs": [],
      "source": [
        "testdf5.shape"
      ],
      "id": "5a8abaa6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba439c0c"
      },
      "source": [
        "## Merging the test data"
      ],
      "id": "ba439c0c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "439bc492"
      },
      "outputs": [],
      "source": [
        "# Merge the first two datasets on a common column\n",
        "merged_test_df = pd.merge(testdf1, testdf2, on='CustomerID')\n",
        "\n",
        "# Merge the third dataset with the merged dataset on a common column\n",
        "merged_test_df = pd.merge(merged_test_df, testdf3, on='CustomerID')\n",
        "\n",
        "# Merge the fourth dataset with the merged dataset on a common column\n",
        "merged_test_df = pd.merge(merged_test_df, testdf4, on='CustomerID')\n",
        "\n",
        "# Merge the fifth dataset with the merged dataset on a common column\n",
        "merged_test_df = pd.merge(merged_test_df, testdf5, on='CustomerID')"
      ],
      "id": "439bc492"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97341da0"
      },
      "outputs": [],
      "source": [
        "merged_test_df.head()"
      ],
      "id": "97341da0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5ffb0ea"
      },
      "outputs": [],
      "source": [
        "merged_test_df.shape"
      ],
      "id": "e5ffb0ea"
    },
    {
      "cell_type": "code",
      "source": [
        "merged_test_df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "lCeMxJdF_i__"
      },
      "id": "lCeMxJdF_i__",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acf85985"
      },
      "outputs": [],
      "source": [
        "merged_test_df.isnull().sum()"
      ],
      "id": "acf85985"
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_test_df.InsuredGender.value_counts())"
      ],
      "metadata": {
        "id": "Gvkyd6UJ_tL_"
      },
      "id": "Gvkyd6UJ_tL_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7f556b5"
      },
      "outputs": [],
      "source": [
        "merged_test_df['InsuredGender'].fillna(merged_test_df['InsuredGender'].mode()[0], inplace=True)"
      ],
      "id": "c7f556b5"
    },
    {
      "cell_type": "code",
      "source": [
        "merged_test_df['Country'].fillna(merged_test_df['Country'].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "efCbq3dw-8Mr"
      },
      "id": "efCbq3dw-8Mr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_test_df.isnull().sum()"
      ],
      "metadata": {
        "id": "1uN6pBwW-8JX"
      },
      "id": "1uN6pBwW-8JX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_unique_columns=[]\n",
        "for i in merged_test_df.columns:\n",
        "    if len(merged_test_df[i].value_counts())==1:\n",
        "        list_unique_columns.append(i)\n",
        "for i in list_unique_columns:\n",
        "    merged_test_df.drop([i],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "DOLtPsen-8Gn"
      },
      "id": "DOLtPsen-8Gn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replace ? with nan\n",
        "merged_test_df=merged_test_df.replace('?',np.nan)"
      ],
      "metadata": {
        "id": "0JiHsg9Q-8Du"
      },
      "id": "0JiHsg9Q-8Du",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in merged_test_df.columns:\n",
        "    print(column, merged_test_df[column][merged_test_df[column] == '?'].count())"
      ],
      "metadata": {
        "id": "-ENRwNh7-8Aj"
      },
      "id": "-ENRwNh7-8Aj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#missing value function\n",
        "def missing_data(data):\n",
        "    total = data.isnull().sum()\n",
        "    percent = (data.isnull().sum()/data.isnull().count()*100)\n",
        "    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "    types = []\n",
        "    for col in data.columns:\n",
        "        dtype = str(data[col].dtype)\n",
        "        types.append(dtype)\n",
        "    tt['Types'] = types\n",
        "    return tt    \n",
        "\n",
        "missing_data(merged_test_df)['Percent'].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "ihKco8N_-79u"
      },
      "id": "ihKco8N_-79u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_test_df['PropertyDamage'].fillna(merged_test_df['PropertyDamage'].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "bb8hX7fU-73e"
      },
      "id": "bb8hX7fU-73e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_test_df['PoliceReport'].fillna(merged_test_df['PoliceReport'].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "IPB5u-Dq-70T"
      },
      "id": "IPB5u-Dq-70T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_test_df['TypeOfCollission'].fillna(merged_test_df['TypeOfCollission'].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "co26vTqb-7xe"
      },
      "id": "co26vTqb-7xe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#missing value function\n",
        "def missing_data(data):\n",
        "    total = data.isnull().sum()\n",
        "    percent = (data.isnull().sum()/data.isnull().count()*100)\n",
        "    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "    types = []\n",
        "    for col in data.columns:\n",
        "        dtype = str(data[col].dtype)\n",
        "        types.append(dtype)\n",
        "    tt['Types'] = types\n",
        "    return tt    \n",
        "\n",
        "missing_data(merged_test_df)['Percent'].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "kgusiEvX-7uc"
      },
      "id": "kgusiEvX-7uc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = list(merged_test_df.columns)\n",
        "print(column_names)"
      ],
      "metadata": {
        "id": "U_j9C18C-7rc"
      },
      "id": "U_j9C18C-7rc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get numerical and categorical column names\n",
        "numerical_cols = merged_test_df.select_dtypes(include='number').columns.tolist()\n",
        "print(\"Numerical columns:\", numerical_cols)"
      ],
      "metadata": {
        "id": "I3Ff9iyFAsbl"
      },
      "id": "I3Ff9iyFAsbl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create box plots for each numerical column\n",
        "for col in numerical_cols:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.boxplot(merged_test_df[col])\n",
        "    plt.title(col)\n",
        "    plt.ylabel('Value')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "i753cPljAvch"
      },
      "id": "i753cPljAvch",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of columns to replace outliers\n",
        "cols_to_replace = ['AmountOfInjuryClaim', 'AmountOfPropertyClaim', 'InsuredAge',\n",
        "                   'CapitalLoss', 'PolicyAnnualPremium', 'UmbrellaLimit']\n",
        "\n",
        "# Define a function to replace outliers with the median value\n",
        "def replace_outliers(merged_test_df, col):\n",
        "    q1 = merged_test_df[col].quantile(0.25)\n",
        "    q3 = merged_test_df[col].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_tail = q1 - 1.5 * iqr\n",
        "    upper_tail = q3 + 1.5 * iqr\n",
        "    med = np.median(merged_test_df[col])\n",
        "    for i in merged_test_df[col]:\n",
        "        if i > upper_tail or i < lower_tail:\n",
        "            merged_test_df[col] = merged_test_df[col].replace(i, med)\n",
        "    return merged_test_df\n",
        "\n",
        "# Replace outliers for each column in the list\n",
        "for col in cols_to_replace:\n",
        "    merged_test_df = replace_outliers(merged_test_df, col)\n",
        "\n",
        "# Plot the boxplots for each column after outlier treatment\n",
        "fig, axs = plt.subplots(5, 2, figsize=(20, 30))\n",
        "axs = axs.ravel()\n",
        "for i, col in enumerate(cols_to_replace):\n",
        "    sns.boxplot(x=col, data=merged_test_df, palette=\"Reds_r\", ax=axs[i])\n",
        "    axs[i].set_title(\"Box Plot for \"+col+\" after median imputation\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jxDf2oqbAzur"
      },
      "id": "jxDf2oqbAzur",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assuming 'df' is the pandas DataFrame containing the data\n",
        "numerical_cols = ['IncidentTime', 'NumberOfVehicles', 'BodilyInjuries', 'AmountOfInjuryClaim', 'AmountOfPropertyClaim', 'AmountOfVehicleDamage', 'InsuredAge', 'InsuredZipCode', 'CapitalGains', 'CapitalLoss', 'InsurancePolicyNumber', 'CustomerLoyaltyPeriod', 'Policy_Deductible', 'PolicyAnnualPremium', 'UmbrellaLimit']\n",
        "\n",
        "unique_counts = merged_test_df[numerical_cols].nunique()\n",
        "print(unique_counts)"
      ],
      "metadata": {
        "id": "eEyM-bpWA4U5"
      },
      "id": "eEyM-bpWA4U5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = merged_test_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "print(\"Categorical columns:\", categorical_cols)"
      ],
      "metadata": {
        "id": "1cTHzTLBBAfM"
      },
      "id": "1cTHzTLBBAfM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the categorical columns\n",
        "categorical_cols = ['CustomerID', 'DateOfIncident', 'TypeOfIncident', \n",
        "                    'SeverityOfIncident', 'AuthoritiesContacted', 'IncidentState', 'IncidentCity', \n",
        "                    'IncidentAddress', 'Witnesses', \n",
        "                    'InsuredGender', 'InsuredEducationLevel', 'InsuredOccupation', \n",
        "                    'InsuredHobbies', 'DateOfPolicyCoverage', 'InsurancePolicyState', 'Policy_CombinedSingleLimit', \n",
        "                    'InsuredRelationship', 'VehicleAttribute', 'VehicleAttributeDetails']\n"
      ],
      "metadata": {
        "id": "X-Y0TX6eBD8f"
      },
      "id": "X-Y0TX6eBD8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = ['CustomerID', 'DateOfIncident', 'IncidentAddress', 'Witnesses', 'PoliceReport',  'InsuredHobbies', 'DateOfPolicyCoverage', 'Policy_CombinedSingleLimit']\n",
        "\n",
        "merged_test_df = merged_test_df.drop(columns=columns_to_drop)"
      ],
      "metadata": {
        "id": "J0JF7XrxBGZM"
      },
      "id": "J0JF7XrxBGZM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary to map category values to numerical labels\n",
        "label_map = {}\n",
        "label_count = 0\n",
        "\n",
        "# Loop through each categorical column\n",
        "for col in categorical_cols:\n",
        "    \n",
        "    # If the column is not in the columns to drop list\n",
        "    if col not in columns_to_drop:\n",
        "        \n",
        "        # Get unique values in the column\n",
        "        unique_vals = merged_test_df[col].unique()\n",
        "        \n",
        "        # Loop through each unique value\n",
        "        for val in unique_vals:\n",
        "            \n",
        "            # If the value has not been seen before, assign a new label\n",
        "            if val not in label_map:\n",
        "                label_map[val] = label_count\n",
        "                label_count += 1\n",
        "                \n",
        "        # Replace categorical values in the column with their corresponding numerical labels\n",
        "        merged_test_df[col] = merged_test_df[col].apply(lambda x: label_map[x])"
      ],
      "metadata": {
        "id": "7oe7-FGQDFVj"
      },
      "id": "7oe7-FGQDFVj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}